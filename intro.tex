In recent years, there has been an explosion of machine learning applications ranging from (partially) self-driving cars to large language models such as ChatGPT. These models are becoming an increasingly prevalent part of society, and it is therefore crucial that they are deployed in a safe and reliable way.

Classical machine learning theory, which forms the basis upon which much of this technology is built, typically involves a formalism in which the learner is given input data and asked to solve a specific task. For example, in the statistical learning framework for classification, the learner is given training data and asked to output a classifier with the highest possible accuracy. However, specific approaches such as this often fail to capture the multitude of behaviors and features that are necessary for broader applications of machine learning. Indeed, performing well on test data is far from a sufficient condition for a model to be deployable in the real world. 

This dissertation attempts towards building theoretical foundations that are able to address machine learning in this broader context by considering two specific problems in reliable machine learning, \textit{adversarial examples,} and \textit{data copying.}


\textbf{Adversarial examples.} Adversarial examples arise in classification, and are small imperceptible changes to a given input that are designed to cause misclassification. For example, a malicious actor might try to circumvent a spam filter by finding perturbations that fool the filter but nevertheless preserve the essence of their content. To remedy this, there has been a growing focus on building \textit{robust} classifiers for which adversarial examples cannot exist.

In the first two chapters of this dissertation, we study this problem in the non-parametric setting and seek to understand under what conditions non-parametric algorithms output classifiers that are both accurate and robust. In the next two chapters, we consider linear classification and seek to understand whether robust classification intrinsically requires more data than simply focusing on accuracy does.

\textbf{Data Copying.} Next, we switch gears and consider a problem in reliable \textit{generative modeling.} Prior work (i.e. \cite{lopez2016revisiting,XHYGSWK18}) has found that large generative models often appear to memorize their training data and often output a near copy of one of their training points when queried. In addition to posing clear security risks with respect to privacy or copyright violations, such "data-copying" also indicates poor generalization and limited usefulness. In the last chapter of this dissertation, we propose a formal definition of data-copying and give the first algorithm with provable guarantees for detecting it. 



%The privacy risks and utility requirements of each of these settings and applications warrant different approaches to privacy-preserving ML. This dissertation proposes a variety of solutions to situations like those above. In doing so, I hope to illuminate the advantage of taking an application-specific approach to both measuring privacy risks and engineering private algorithms. The following five chapters are roughly organized into two parts: the first two chapters cover \emph{empirical} privacy methods, and the remaining three cover \emph{formal} privacy methods. The former includes statistical tests to quantify privacy risks of large ML models. The latter proposes provably private algorithms to satisfy different privacy definitions chosen for different ML tasks. 
%
%\textbf{Empirical methods.} The first two chapters explore empirically measuring privacy risks in the domain of vision modeling. In both chapters, we analyze to what extent large vision models \emph{memorize} their training images, and thereby risk exposing them. In contrast with the following three chapters, we are not proposing a provably privacy-preserving algorithm. Instead we are designing methodical empirical tests to quantify memorization. 
%
%\textbf{Provably private algorithms.} The final three chapters propose algorithms that allow us to share our data in a provably private way. We explore privacy preserving algorithms in the text, location, and interpersonal-correlated domains (\emph{e.g.} social networks or genetically linked medical data). For each of these, we study different ML tasks and privacy risks to motivate different privacy definitions and provably private algorithms. 
%
%Taken together, this document offers a mindset towards practicable privacy methods. By directly considering the data domain and the task at hand, it is possible to efficiently measure privacy risks and propose provably private algorithms while still completing the learning task at hand .