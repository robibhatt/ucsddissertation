\graphicspath{{./chapters/chapter2/}}
%\newtheorem{thm}{Theorem}
%\newtheorem{lem}[thm]{Lemma}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

\def\D{{\mathcal D}}
\def\Pphi{\overline{\Phi}}
\def\F{{\mathcal F}}
\def\N{{\mathcal N}}
%\def\R{{\mathbb R}}
\def\E{{\mathbb E}}
\def\A{\Pi}
\def\B{\Sigma}
\def\diam{\text{diam}}
\def\c{\mathcal L}
\def\l{\ell}
\def\seq{seq}
\def\R{\mathbb{R}}
\def\C{\mathcal C}
\def\p{p}
\def\s{size}
\def\L{\mathcal L}
\def\o{opt}
\def\H{\mathcal H}
\def\calH{\mathcal H}
\def\of{approxCluster}
\def\on{onlineCluster}
\def\R{\mathbb R}
\def\Y{\{\pm 1\}}
\def\U{\mathbb U}
\def\dd{\Delta}
\def\simp{{U\Delta}}
\def\g{g}
\def\rr{R}
\def\f{f}


\chapter{Sample Complexity of Robust Linear Classification on Separated Data} 

\section{Introduction}

Motivated by the use of machine learning in safety-critical settings, adversarially robust classification has been of much recent interest. Formally, the problem is as follows. A learner is given training data drawn from an underlying distribution $D$, a hypothesis class $\calH$, a robustness metric $d$, and a radius $r$. The learner's goal is to find a classifier $h \in \calH$ which has the lowest robust loss at radius $r$. The robust loss of a classifier is the expected fraction of examples where either $f(x) \neq y$ or where there exists an $x'$ at distance $d(x, x') \leq r$ such that $f(x) \neq f(x')$.  Robust classification thus aims to find a classifier that maximizes accuracy on examples that are distance $r$ or more from the decision boundary, where distances are measured according to the metric $d$.


In this work, we ask: how many samples are needed to learn a classifier with low robust loss when $\calH$ is the class of linear classifiers, and $d$ is an $\ell_p$-metric? Prior work has provided both upper~\cite{bartlett19, ravikumar20} as well as lower bounds~\cite{Schmidt18, ravikumar20} on the sample complexity of the problem. However, almost all look at settings where the data distribution itself is not separated --  data from different classes overlap or are close together in space. In this case, the classifier that minimizes robust loss is quite different from the one that minimizes error, which often leads to strong sample complexity gaps. Many real tasks where robust solutions are desired however tend to involve well-separated data~\cite{Yang20}, and hence it is instructive to look at what happens in these cases.

With this motivation, we consider in this work robust classification of data that is linearly $r$-separable. Specifically, there exists a linear classifier which has zero robust loss at robustness radius $r$. This case is thus the analog of the realizable case for robust classification, and we consider both upper and lower bounds in this setting.

For lower bounds, prior work \cite{Cullina18} shows that both standard and robust linear classification have VC-dimension $O(d)$, and consequently have similar bounds on the expected loss in the worst case. However, these results do not apply to this setting since we are specifically considering well-separated data, which greatly restricts the set of possible worst-case distributions.  For our lower bound, we provide a family of distributions that are linearly $r$-separable and where the maximum margin classifier, given $n$ independent samples, has error $O(1/n)$. In contrast, any algorithm for finding the minimum robust loss classifier has robust loss at least $\Omega(d/n)$, where $d$ is the data dimension. These bounds hold for all $\ell_p$-norms provided $p > 1$, including $p=2$ and $p=\infty$. Unlike prior work, our bounds do not rely on the difference in loss between the solutions with optimal robust loss and error, and hence cannot be obtained by prior techniques. Instead, we introduce a new geometric construction that exploits the fact that learning a classifier with low robust loss when data is linearly $r$-separated requires seeing a certain number of samples close to the margin.

For upper bounds, prior work \cite{bartlett19} provides a bound on the Rademacher complexity of adversarially robust learning, and show that it can be worse than the standard Rademacher complexity by a factor of $d^{1/q}$ for $\ell_p$-norm robustness where $1/p + 1/q = 1$. Thus, an interesting question is whether dimension-independent bounds, such as those for the accuracy under large margin classification, can be obtained for robust classification as well. Perhaps surprisingly, we show that when data is really well-separated, the answer is yes. Specifically, if the data distribution is linearly $r + \gamma$-separable, then there exists an algorithm that will find a classifier with robust loss $O(\Delta^2/\gamma^2 n)$ at radius $r$ where $\Delta$ is the diameter of the instance space. Observe that much like the usual sample complexity results on SVM and perceptron, this upper bound is independent of the data dimension and depends only on the excess margin (over $r$). This establishes that when data is really well-separated, finding robust linear classifiers does not require a very large number of samples. 

While the main focus of this work is on linear classifiers, we also show how to generalize our upper bounds to Kernel Classification, where we find a similar dynamic with the loss being governed by the excess margin in the embedded kernel space. However, we defer a thorough investigation of robust kernel classification as an avenue for future work.

Our results imply that while adversarially robust classification may be more challenging than simply accurate classification when the classes overlap, the story is different when data is well-separated. Specifically, when data is linearly (exactly) $r$-separable, finding an $r$-separated solution to robust loss $\epsilon$ may require $\Omega(d/\epsilon)$ samples for some distribution families where finding an accurate solution is easier. Thus in this case, there is a gap between the sample complexities of robust and simply accurate solutions, and this is true regardless of the $\ell_p$ norm in which robustness is measured. In contrast, if data is even more separated -- linearly $r + \gamma$-separable --  then we can obtain a dimension-independent upper bound on the sample complexity, much like the sample complexity of SVMs and perceptron. Thus, how separable the data is matters for adversarially robust classification, and future works in the area should consider separability while discussing the sample complexity.

\subsection{Related Work}

There is a large body of work \cite{Carlini17, Liu17, Papernot17, Papernot16, Szegedy14, Hein17, Katz17, Wu16,Steinhardt18, Sinha18} empirically studying adversarial examples primarily in the context of neural networks. Several works \cite{Schmidt18, Raghunathan20, Tsipras19} have empirically investigated trade-offs between robust and standard classification.

On the theoretical side, this phenomenon has been studied in both the parametric and non-parametric settings. On the parametric side, several works \cite{loh18, attias19, Srebro19, bartlett19, pathak20} have focused on finding distribution agnostic bounds of the sample complexity for robust classification. In \cite{Srebro19}, Srebro et. al. showed through an example that the VC dimension of robust learning may be much larger than standard or accurate learning indicating that the sample complexity bounds may be higher. However, their example did not apply to linear classifiers. 

\cite{Kane20} considers learning linear classifiers robustly, but is primarily focused on computational complexity as opposed to sample complexity.

In \cite{bartlett19}, Bartlett et. al. investigated the Rademacher complexity of robustly learning linear classifiers as well as neural networks. They showed that in both cases, the robust Rademacher complexity can be bounded in terms of the dimension of the input space -- thus indicating a possible gap between standard and robust learning. However, as with the works considering VC dimension, this work is fundamentally focused on upper bounds  -- they do not show true lower bounds on data requirements.

Because of it's simplicity and elegance, the case where the data distribution is a mixture of Gaussians has been particularly well-studied. The first such work was \cite{Schmidt18}, in which Schmidt et. al. showed an $\Omega(\sqrt{d})$ gap between the standard and robust sample complexity for a mixture of two Gaussians using the $\ell_\infty$ norm. This was subsequently expanded upon in \cite{Bhagoji19}, \cite{robey20} and  \cite{ravikumar20}. \cite{Bhagoji19} introduces a notion of ``optimal transport," which they subsequently apply to the Gaussian case, deriving a closed form expression for the optimally robust linear classifier. Their results apply to any $\ell_p$ norm. \cite{robey20} applies expands upon \cite{Schmidt18} by consider mixtures of three Gaussians in both the $\ell_2$ and $\ell_\infty$ norms. Finally, \cite{ravikumar20} fully generalizes the results of \cite{Schmidt18} providing tight upper and lower bounds on the standard and robust sample complexities of a mixture of two Gaussians, in any norm (including $\ell_p$ for $p \in [1, \infty]$). \cite{Schmidt18} and \cite{ravikumar20} bear the most relevance with our work, and we consequently carefully compare our results in section \ref{sec:comparison}.

Another approach for lower and upper bounds on sample complexities for linear classifiers can be found in \cite{Cullina18}, which examines the robust VC dimension of learning linear classifiers. They show that the VC dimension is $d+1$, just as it is in the standard case. This implies that the bounds in the robust case match the bounds in the standard case and in particular shows a lower bound of $\Omega(d/n)$ on the expected loss of learning a robust linear classifier from $n$ samples.

While this result appears to match our lower bound, there is a crucial distinction between the bounds. Our bound implies that there exists some distribution with a large $\ell_2$ margin for which the expected robust loss must be $\Omega(d/n)$. On the other hand, standard results about learning linear classifiers on large margin data implies that the expected standard loss will be $O(1/n)$ (when running the max-margin algorithm). For this reason, our paper provides a case in the well-separated setting in which learning linear classifiers is provably more difficult (in terms of sample complexity) in the robust setting than in the standard setting. By contrast, \cite{Cullina18} does not show this. Their paper only implies (through standard VC constructions) the existence of \textit{some} distribution that is difficult to learn, and the standard PAC bounds cannot ensure that such a distribution also has a large $\ell_2$ margin.

In the non-parametric setting, there are several works which contrast standard learning with robust learning. \cite{WJC18} considers the nearest neighbors algorithm, and shows how to adapt it for converging towards a robust classifier. In \cite{YRWC19}, Yang et. al. propose the $r$\textit{-optimal classifier}, which is the robust analog of the Bayes optimal classifier. Through several examples they show that it is often a fundamentally different classifier - which can lead to different convergence behavior in the standard and robust settings. \cite{Bhattacharjee20} unified these approaches by specifying conditions under which non-parametric algorithms can be adapted to converge towards the $r$-optimal classifier, thus introducing $r$-consistency, the robust analog of consistency.

\section{Preliminaries}
We consider binary classification over $\R^d \times \Y$. Our metric of choice is the $\ell_p$ norm, where $p > 1$ (including $p = \infty$) is arbitrary. For $x \in \R^d$, we will use $||x||_p$ to denote the $\ell_p$ norm of $x$, and consequently will use $||x - y ||_p$ to denote the $\ell_p$ distance between $x$ and $y$. We will also let $\ell_q$ denote the dual norm to $\ell_p$ - that is, $\frac{1}{q} + \frac{1}{p}= 1$.

 We use $B_p(x,r)$ to denote the closed $\ell_p$ ball with center $x$ and radius $r$. For any $S \subset \R^d$, we let $diam_p(S)$ denote its diameter: that is, $diam_p(S) = \sup_{x, y \in S} ||x - y||_p.$

\subsection{Standard and Robust Loss}

In classical statistical learning, the goal is to learn an accurate classifier, which is defined as follows:

\begin{defn}
Let $\D$ be a distribution over $\R^d \times \Y$, and let $f \in \Y^{\R^d}$ be a classifier. Then the \textbf{standard loss} of $f$ over $\D$, denoted $\L(f, \D)$, is the fraction of examples $(x,y) \sim \D$ for which $f$ is not accurate. Thus $$\L(f, \D) = P_{(x,y) \sim \D}[f(x) \neq y].$$
\end{defn}

Next, we define robustness, and the corresponding robust loss.

\begin{defn}
A classifier $f \in \Y^{\R^d}$ is said to be \textbf{robust} at $x$ with radius $r$ if $f(x) = f(x')$ for all $x' \in B_p(x,r)$. 
\end{defn}


\begin{defn}
The \textbf{robust loss} of $f$ over $\D$, denoted $\L_r(f, \D)$, is the fraction of examples $(x,y) \sim \D$ for which $f$ is either inaccurate at $(x,y)$, or $f$ is not robust at $(x,y)$ with radius $r$. Observe that this occurs if and only if there is some $x' \in B_p(x,r)$ such that $f(x') \neq y$. Thus $$\L_r(f, \D) = P_{(x, y) \sim \D}[\exists x' \in B_p(x,r)\text{ s.t. }f(x') \neq y].$$ 
\end{defn}

\subsection{Expected Loss and  Sample Complexity}

The most common way to characterize the performance of a learning algorithm is through an $(\epsilon, \delta)$ guarantee, which computes $\epsilon_n, \delta_n$ such that an algorithm trained over $n$ samples has loss at most $\epsilon_n$ with probability at least $1 - \delta_n$. 

In this work, we use the simpler notion of \textit{expected loss}, which is defined as follows: 

\begin{defn}
Let $A$ be a learning algorithm and let $\D$ be a distribution over $\R^d \times \{\pm 1\}$. For any $S \sim \D^n$, we let $A_S$ denote the classifier learned by $A$ from training data $S$. Then the \textbf{expected standard loss} of $A$ with respect to $\D$, denoted $EL^n(A, \D)$ where $n$ is the number of training samples, is defined as $$E\L^n(A, \D) = \E_{S \sim \D^n} \L(A_S, \D).$$ Similarly, we define the \textbf{expected robust loss} of $A$ with respect to $\D$ as $$E\L_r^n(A , \D) = \E_{S \sim \D^n}\L_r(A_S, \D).$$ 
\end{defn}

Our main motivation for using this criteria is simplicity. Our primary goal is to compare and contrast the performances of algorithms in the standard and robust cases, and this contrast clearest when the performances are summarized as a single number (namely the expected loss) rather than an $(\epsilon, \delta)$ pair. 

Next, we address the notion of sample complexity. As above, sample complexity is typically defined as the minimum number of samples needed to guarantee $(\epsilon, \delta)$ performance. In this work, we will instead define it solely with respect to $\epsilon$, the expected loss. 

\begin{defn}
Let $\D$ be a distribution over $\R^d \times \{\pm 1\}$ and $A$ be a learning algorithm. Then the \textbf{standard sample complexity} of $A$ with respect to $\D$, denoted $m^\epsilon(A, \D)$, is the minimum number of training samples needed such that $A$ has  expected standard loss at most $\epsilon$. Formally, $$m^\epsilon(A, \D) = \min(\{n: E\L^n(A, D) \leq \epsilon\}).$$ Similarly, we can define the \textbf{robust sample complexity} as $$m_r^\epsilon(A, \D) = \min(\{n: E\L^n(A, D) \leq \epsilon\}).$$
\end{defn}



%\input{chapters/chapter1/macros}
%\input{chapters/chapter1/introduction}
%\input{chapters/chapter1/related}
%\input{chapters/chapter1/definition}
%\input{chapters/chapter1/quantifying}
%\input{chapters/chapter1/visualizing}
%\input{chapters/chapter1/mitigation}