\graphicspath{{./chapters/chapter1}}
\chapter{When are Non-Parametric Methods Robust?} 

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}{Example}

\def\D{{\mathcal D}}
\def\X{\mathcal X}
\def\R{\mathbb R}
\def\Y{\{\pm 1\}}
\def\w{\hat{w}}
\def\P{\mathbb{P}}
\def\I{\hat{I}}
\def\b{g^*}
\def\r{\rho}
\def\rcons{r-consistent}
\def\rconsy{r-consistency}
\def\ap{AdvPrun}
\def\ga{RobustNonPar}

\section{Introduction}

Recent work has shown that many classifiers tend to be highly non-robust and that small strategic modifications to regular test inputs can cause them to misclassify~\cite{Szegedy14, Goodfellow14, MeekLowd05}. Motivated by the use of machine learning in safety-critical applications, this phenomenon has recently received considerable interest; however, what exactly causes this phenomenon -- known in the literature as {\em{adversarial examples}} -- still remains a mystery.

Prior work has looked at three plausible reasons why adversarial examples might exist. The first, of course, is the possibility that in real data distributions, different classes are very close together in space -- which does not seem plausible in practice. Another possibility is that classification algorithms may require more data to be robust than to be merely accurate; some prior work~\cite{Madry18, WJC18, Srebro19} suggests that this might be true for certain classifiers or algorithms. Finally, others~\cite{Bubeck19, Vinod19, WJC18} have suggested that better training algorithms may give rise to more robust classifiers -- and that in some cases, finding robust classifiers may even be computationally challenging.

In this work, we consider this problem in the context of general non-parametric classifiers. Contrary to parametrics, non-parametric methods are a form of local classifiers, and include a large number of pattern recognition methods such as nearest neighbors, decision trees, random forests and kernel classifiers. There is a richly developed statistical theory of non-parametric methods~\cite{devroye96}, which focuses on accuracy, and provides very general conditions under which these methods converge to the Bayes optimal with growing number of samples. We, in contrast, analyze robustness properties of these methods, and ask instead when they converge to the classifier with the highest astuteness at a desired radius $r$. Recall that the astuteness of a classifier at radius $r$ is the fraction of points from the distribution on which it is accurate and has the same prediction up to a distance $r$~\cite{WJC18, Madry18}.

 We begin by looking at the very simple case when data from different classes is well-separated -- by at least a distance $2r$. Although achieving astuteness in this case may appear trivial, we show that even in this highly favorable case, not all non-parametric methods provide robust classifiers -- and this even holds for methods that converge to the Bayes optimal in the large sample limit.  

This raises the natural question -- when do non-parametric methods produce astute classifiers? We next provide conditions under which a non-parametric method converges to the most astute classifier in the large sample limit under well-separated data. Our conditions are analogous to the classical conditions for convergence to the Bayes optimal~\cite{devroye96, Stone77}, but a little stronger. We show that nearest neighbors and kernel classifiers whose kernel functions decay fast enough, satisfy these conditions, and hence converge to astute classifiers in the large sample limit. In constrast, histogram classifiers, which do converge to the Bayes optimal in the large sample limit, may not converge to the most astute classifier. This indicates that there may be some non-parametric methods, such as nearest neighbors and kernel classifiers, that are more naturally robust when trained on well-separated data, and some that are not.

What happens when different classes in the data are not as well-separated? For this case, \cite{YRWC19} proposes a method called Adversarial Pruning that preprocesses the training data by retaining the maximal set of points such that different classes are distance $\geq 2r$ apart, and then trains a non-parametric method on the pruned data. We next prove that if a non-parametric method has certain properties, then the classifier produced by Adversarial Pruning followed by the method does converges to the most astute classifier in the large sample limit. We show that again nearest neighbors and kernel classifiers whose kernel functions decay faster than inverse polynomials satisfy these properties. Our results thus complement and build upon the empirical results of~\cite{YRWC19} by providing a performance guarantee. 

What can we conclude about the cause for adversarial examples? Our results seem to indicate that at least for non-parametrics, it is mostly the training algorithms that are responsible. With a few exceptions, decades of prior work in machine learning and pattern recognition has largely focussed on designing training methods that provide increasingly accurate classifiers -- perhaps to the detriment of other aspects such as robustness. In this context, our results serve to (a) provide a set of guidelines that can be used for designing non-parametric methods that are robust and accurate on well-separated data and (b) demonstrate that when data is not well-separated, preprocessing through adversarial pruning~\cite{YRWC19} may be used to ensure convergence to optimally astute solutions in the large sample limit. 

\subsection{Related Work}

There is a large body of work on adversarial attacks~\cite{Carlini17, Liu17, Papernot17, Papernot16,Szegedy14} and defenses~\cite{Hein17,Katz17,Schmidt18,Wu16,Steinhardt18, Sinha18} in the parametric setting, specifically focusing on neural networks. On the other hand, adversarial examples for nonparametric classifiers have mostly been studied in a much more ad-hoc manner, and to our knowledge, there has been no theoretical investigation into general properties of algorithms that promote robustness in non-parametric classifiers.

For nearest neighbors, there has been some prior work on adversarial attacks~\cite{Amsaleg17, Sitawarin19, WJC18, YRWC19} as well as defenses. Wang et. al. \cite{WJC18} proposes a defense for 1-NN by pruning the input sample. However, their defense learns a classifier whose robustness regions converge towards those of the Bayes optimal classifier, which itself may potentially have poor robustness properties. Yang et. al. \cite{YRWC19} accounts for this problem by proposing the notion of the $r$-optimal classifier, and propose an algorithm called Adversarial Pruning which can be interpreted as a finite sample approximation to the $r$-optimal. However, they do not provide formal performance guarantees for Adversarial Pruning, which we do. 

For Kernel methods, Hein and Andriushchenko \cite{Hein17} study lower bounds on the norm of the adversarial manipulation that is required for changing a classifiers output. They specifically study bounds for Kernel Classifiers, and propose an empirically based regularization idea that improves robustness. In this work, we improve the robustness properties of kernel classification through adversarial pruning, and show formal guarantees regarding convergence towards the $r$-optimal classifier. 

For decision trees and random forests, attacks and defenses have been provided by \cite{Hein19, Kantchelian15, Hsiehicml19}. Again, most of the work here is empirical in nature, and convergence guarantees are not provided. 

Pruning has a long history of being applied for improving nearest neighbors \cite{Gates72, Gottlieb14, Hart68, KontorovichSW17, KontorovichW15, Hanneke19}, but this has been entirely done in the context of generalization, without accounting for robustness. In their work, Yang et. al. empirically show that adversarial pruning can improve robustness for nearest neighbor classifiers. However, they do not provide any formal guarantees for their algorithms. In this work, we prove formal guarantees for \textit{adversarial pruning} in the large sample limit, both for nearest neighbors as well as for more general \textit{weight functions.} 

There is a long history of literature for understanding the consistency of Kernel classifiers \cite{Steinwart05, Stone77}, but this has only been done for accuracy and generalization. In this work, we find different conditions are needed to ensure that a Kernel classifier converges in robustness in addition to accuracy.

\section{Preliminaries}

\subsection{Setting}
We consider binary classification where instances are drawn from a totally bounded metric space $\X$ that is equipped with distance metric denoted by $d$, and the label space is $\Y = \{ -1, +1 \}$. The classical goal of classification is to build a highly \textit{accurate} classifier, which we define as follows.

\begin{defn}
(Accuracy) Let $\D$ be a distribution over $\X \times \Y$, and let $f \in \Y^\X$ be a classifier. Then the \textbf{accuracy} of $f$ over $\D$, denoted $A(f, \D)$, is the fraction of examples $(x,y) \sim \D$ for which $f(x) = y$. Thus $$A(f, \D) = P_{(x,y) \sim \D}[f(x) = y].$$
\end{defn}

In this work, we consider \textit{robustness} in addition to accuracy. Let $B(x,r)$ denoted the closed ball of radius $r$ centered at $x$. 

\begin{defn}
(Robustness) A classifier $f \in \Y^\X$ is said to be \textbf{robust} at $x$ with radius $r$ if $f(x) = f(x')$ for all $x' \in B(x,r)$.
\end{defn}

Our goal is to find non-parametric algorithms that output classifiers that are robust, in addition to being accurate. To account for both criteria, we combine them into a notion of \textit{astuteness}~\cite{WJC18, Madry18}. 

\begin{defn}
(Astuteness) A classifier $f \in \Y^\X$ is said to be \textbf{astute} at $(x,y)$ with radius $r$ if $f$ is robust at $x$ with radius $r$ and $f(x) = y$. The \textbf{astuteness} of $f$ over $\D$, denoted $A_r(f, \D)$, is the fraction of examples $(x,y) \sim \D$ for which $f$ is astute at $(x,y)$ with radius $r$. Thus $$A_r(f, \D) = P_{(x, y) \sim \D}[f(x') = y, \forall x' \in B(x,r)].$$
\end{defn}

It is worth noting that $A_0(f, \D) = A(f, \D)$, since astuteness with radius $0$ is simply the accuracy. For this reason, we will use $A_0(f, \D)$ to denote accuracy from this point forwards.

\subsection{Notions of Consistency}

Traditionally, a classification algorithm is said to be consistent if as the sample size grows to infinity, the accuracy of the classifier it learns converges towards the best possible accuracy on the underlying data distribution. We next introduce and formalize an alternative form of consistency, called $r$-consistency, that applies to robust classifiers.

We begin with a formal definition of the Bayes Optimal Classifier -- the most accurate classifier on a distribution -- and consistency. 

\begin{defn}
(Bayes Optimal Classifier) The \textbf{Bayes Optimal Classifier} on a distribution $\D$, denoted by $\b$, is defined as follows. Let $\eta(x) = p_\D(+1|x)$. Then
 \[ \b(x) = \begin{cases} 
      +1 & \eta(x) \geq 0.5 \\
      -1 & \eta(x) < 0.5 \\
   \end{cases}
\]
It can be shown that $\b$ achieves the highest accuracy over $\D$ over all classifiers.
\end{defn}

\begin{defn}
(Consistency) Let $M$ be a classification algorithm  over $\X \times \Y$. $M$ is said to be \textbf{consistent} if for any $\D$ over $\X \times \Y$, and any $\epsilon, \delta$ over $(0,1)$, there exists $N$ such that for $n \geq N$, with probability $1-\delta$ over $S \sim \D^n$, we have: $$A(M(S), \D) \geq A(\b, \D) - \epsilon,$$ where $\b$ is the Bayes optimal classifier for $\D$. 
\end{defn}

How can we incorporate robustness in addition to accuracy in this notion? A plausible way, as used in~\cite{WJC18}, is that the classifier should converge towards being astute where the Bayes Optimal classifier is astute. However, the Bayes Optimal classifier is not necessarily the most astute classifier and may even have poor astuteness. To see this, consider the following example. 

\paragraph{Example 1}
Consider $\D$ over $\X = [0,1]$ such that $\D_\X$ is the uniform distribution and $$p(y=1|x) = \frac{1}{2} + \sin \frac{4 \pi x}{r}.$$ For any point $x$, there exists $x_1, x_2 \in ([x-r, x+r] \cap [0,1])$ such that $p(y=1|x_1) > \frac{1}{2}$ and $p(y=1|x_2) < \frac{1}{2}$. $A_r(\b, r) = 0$. However, the classifier that always predicts $f(x) = +1$ does better. It is robust everywhere, and since $P_{(x,y) \sim \D}[y = +1] = \frac{1}{2}$, it follows that $A_r(f, \D) = \frac{1}{2}$. \\ \\

This motivates the notion of the $r$-optimal classifier, introduced by~\cite{YRWC19}, which is the classifier with maximum astuteness. 

\begin{defn}
($r$-optimal classifier) The \textbf{$r$-optimal classifier} of a distribution $G$ denoted by $\b_r$ is the classifier with maximum astuteness. Thus $$\b_r = \argmax_{f \in \Y^\X} A_r(f, \D).$$ We let $A_r^*(\D)$ denote $A_r(\b_r, \D)$. 
\end{defn}

Observe that $\b_r$ is not necessarily unique. To account for this, we use $A_r^*(\D)$ in our definition for \rconsy. 

\begin{defn} \label{defn_archons}
(\rcons) Let $M$ be a classification algorithm over $\X \times \Y$. $M$ is said to be \textbf{\rcons} if for any $\D$,  any $\epsilon, \delta \in (0,1)$, and $0 < \gamma < r$, there exists $N$ such that for $n \geq N$, with probability $1-\delta$ over $S \sim \D^n$, $$A_{r-\gamma}(M(S), \D) \geq A_r^*(\D) - \epsilon.$$ if the above conditions hold for a specific distribution $\D$, we say that $M$ is \rcons\emph{ }with respect to $\D$. 
\end{defn}

Observe that in addition to the usual $\epsilon$ and $\delta$, there is an extra parameter $\gamma$ which measures the gap in the robustness radius. We may need this parameter as when classes are exactly $2r$ apart, we may not be able to find the exact robust boundary with only finite samples. 

Our analysis will be centered around understanding what kinds of algorithms $M$ provide highly astute classifiers for a given radius $r$. We begin by first considering the special case of \textit{$r$-separated} distributions. 

\begin{defn}
($r$-separated distributions) A distribution $\D$ is said to be \textbf{$r$-separated} if there exist subsets $T^+, T^- \subset \X$ such that 
\begin{enumerate}
	\item $\P_{(x,y) \sim \D}[x \in T^y] = 1$. 
	\item $\forall x_1 \in T^+, \forall x_2 \in T^-$, $d(x_1, x_2) > 2r$.
\end{enumerate}
\end{defn}

Observe that if $\D$ is $r$-separated, $A_r(\b_r, \D) = 1$.

\subsection{Non-parametric Classifiers}\label{classifiers}

Many non-parametric algorithms classify points by averaging labels over a local neighborhood from their training data. A very general form of this idea is encapsulated in \textit{weight functions} -- which is the general form we will use.

\begin{defn} \label{def:weight}
\cite{devroye96} A \textbf{weight function} $W$ is a non-parametric classifier with the following properties.
\begin{enumerate}
	\item Given input $S = \{(x_1, y_1), (x_2, y_2,), \dots, (x_n, y_n)\} \sim \D^n$, $W$ constructs functions $w_1^S, \dots, w_n^S: \X \to [0, 1]$ such that for all $x \in \X$, $\sum_1^n w_i^S(x) = 1$. The functions $w_i^S$ are allowed to depend on $x_1, x_2, \dots x_n$ but must be independent of $y_1, y_2, \dots, y_n$. 
	\item $W$ has output $W_S$ defined as \[ W_S(x) = \begin{cases} 
      +1 & \sum_1^n w_i^S(x)y_i > 0 \\
      -1 & \sum_1^n w_i^S(x)y_i \leq 0 \\
   \end{cases}
\]
As a result, $w_i^S(x)$ can be thought of as the weight that $(x_i, y_i)$ has in classifying $x$.
\end{enumerate}
\end{defn}

Weight functions encompass a fairly extensive set of common non-parametric classifiers, which is the motivation for considering them. We now define several common non-parametric algorithms that can be construed as weight functions. 

\begin{defn}
A \textbf{histogram classifier}, $H$, is a non-parametric classification algorithm over $\R^d \times \Y$ that works as follows. For a distribution $\D$ over $\R \times \Y$, $H$ takes $S = \{(x_i, y_i): 1 \leq i \leq n\} \sim \D^n$ as input. Let $k_i$ be a sequence with $\lim_{i \to \infty} k_i = \infty$ and $\lim_{i \to \infty} \frac{k_i}{i} = 0$. $H$ constructs a set of hypercubes $C = \{c_1, c_2, \dots, c_m\}$ as follows:
\begin{enumerate}
	\item Initially $C = \{c\}$, where $S \subset c$.
	\item For $c \in C$, if $c$ contains more than $k_n$ points of $S$, then partition $c$ into $2^d$ equally sized hypercubes, and insert them into $C$.
	\item Repeat step $2$ until all cubes in $C$ have at most $k_n$ points. 
\end{enumerate}
For $x \in \R$ let $c(x)$ denote the unique cell in $C$ containing $x$. If $c(x)$ doesn't exist, then $H_S(x) = -1$ by default. Otherwise, \[ H_S(x) = \begin{cases} 
      +1 & \sum_{x_i \in c(x)} y_i > 0 \\
      -1 & \sum_{x_i \in c(x)}y_i \leq 0 \\
   \end{cases}.
\]
\end{defn}

Histogram classifiers are weight functions in which all $x_i$ contained within the same cell as $x$ are given the same weight $w_i^S(x)$ in predicting $x$, while all other $x_i$ are given weight $0$. 

\begin{defn}
A \textbf{kernel classifier} is a weight function $W$ over $\X \times \Y$ constructed from function $K: \R^+ \cup \{0\} \to \R^+$ and some sequence $\{h_n\} \subset \R^+$ in the following manner. Given $S = \{(x_i, y_i)\} \sim \D^n$, we have $$w_i^S(x) = \frac{K(\frac{d(x, x_i)}{h_n})}{\sum_{j = 1}^n K(\frac{d(x, x_j)}{h_n})}.$$ Then, as above, $W$ has output \[ W_S(x) = \begin{cases} 
      +1 & \sum_1^n w_i^S(x)y_i > 0 \\
      -1 & \sum_1^n w_i^S(x)y_i \leq 0 \\
   \end{cases}
\]
\end{defn}

Finally, we note that $k_n$-nearest neighbors is also a weight function; $w_i^S(x) = \frac{1}{k_n}$ if $x_i$ is one of the $k_n$ closest neighbors of $x$ and $0$ otherwise.

\section{Warm Up: $r$-separated distributions}

We begin by considering the case when the data distribution is $r$-separated; the more general case is considered in Section~\ref{sec:general}. While classifying $r$-separated distributions robustly may appear almost trivial, learning an arbitrary classifier does not necessarily produce an astute result. To see this, consider the following example of a histogram classifier -- which is known to be consistent. 


We let $H$ denote the histogram classifier over $\R$.

\paragraph{Example 2}
Consider the data distribution $\D = \D^+ \cup \D^-$ where $D^+$ is the uniform distribution over $[0, \frac{1}{4})$ and $D^-$ is the uniform distribution over $(\frac{1}{2}, 1]$, $p(+1|x) = 1$ for $x \in \D^+$, and $p(-1|x) = 1$ for $x \in \D^-$. 

We make the following observations (refer to Figure \ref{fig:histogram}).
\begin{enumerate}
	\item $\D$ is $0.1$-separated, since the supports of $\D^+$ and $\D^-$ have distance $0.25 > 0.2$. 
	\item If $n$ is sufficiently large, $H$ will construct the cell $[0.25, 0.5)$, which will not be split because it will never contain any points. 
	\item $H_S(x) = -1$ for $x \in [0.25, 0.5)$.
	\item $H_S$ is not astute at $(x,1)$ for $x \in (0.15, 0.25)$. Thus $A_{0.1}(H_S, \D) = 0.8$.
\end{enumerate}

\begin{figure}
\centering
\definecolor{dtsfsf}{rgb}{0.8274509803921568,0.1843137254901961,0.1843137254901961}
\definecolor{sexdts}{rgb}{0.1803921568627451,0.49019607843137253,0.19607843137254902}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-4.68,0) rectangle (2.46,3);
\draw [line width=2pt] (-3.8971542001619626,1.7601524817792173)-- (-3.90319974718047,1.0800284421971598);
\draw [line width=2pt] (-2.7721542001619626,1.7501524817792171)-- (-2.7781997471804694,1.0700284421971598);
\draw [line width=2pt] (-1.6471542001619623,1.7401524817792173)-- (-1.6531997471804694,1.0600284421971597);
\draw [line width=2pt] (0.6028457998380374,1.720152481779217)-- (0.5968002528195302,1.0400284421971597);
\draw [line width=2pt] (-2.775,1.43)-- (-1.65,1.42);
\draw [line width=2pt,color=sexdts] (-1.65,1.42)-- (0.6,1.4);
\draw [line width=2pt,color=sexdts] (-3.9,1.44)-- (-3.2002330680045032,1.4337798494933733);
\draw [line width=2pt,color=dtsfsf] (-3.2002330680045032,1.4337798494933733)-- (-2.775,1.43);
\draw (-3.54,2.48) node[anchor=north west] {$\mathcal{D}^+$};
\draw (-0.76,2.46) node[anchor=north west] {$\mathcal{D}^-$};
\draw (-4,1.1) node[anchor=north west] {0};
\draw (-3.14,1.08) node[anchor=north west] {0.25};
\draw (-1.86,1.1) node[anchor=north west] {0.5};
\draw (0.5,1.08) node[anchor=north west] {1};
\end{tikzpicture}
\caption{$H_S$ is astute in the green region, but not robust in the red region.} \label{fig:histogram}
\end{figure}

Example 2 shows that histogram classifiers do not always learn astute classifiers even when run on $r$-separated distributions. This motivates the question: which non-parametric classifiers do?

We answer this question in the following theorem, which gives sufficient conditions for a weight function (definition \ref{def:weight}) to be $r$-consistent over an $r$-separated distribution.

\begin{thm}\label{thm_stone_cons}
Let $\D$ be a distribution over $\X \times \Y$, and let $W$ be a weight function. Let $X$ be a random variable with distribution $\D_\X$, and $S = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \sim \D^n$. Suppose that for any $0 < a < b,$ $$\lim_{n \to \infty} \mathbb{E}_{X, S} \big [ \sup_{x' \in B(X, a)} \sum_1^n w_i^S(x')I_{||x_i - x'|| > b} \big] = 0.$$  Then if $\D$ is $r$-separated, $W$ is \rcons\emph{ } with respect to $\D$.  
\end{thm}

First, we compare Theorem \ref{thm_stone_cons} to Stone's theorem \cite{Stone77}, which gives sufficient conditions for a weight function to be consistent (i.e. converge in accuracy towards the Bayes optimal). For convenience, we include a statement of Stone's theorem. 
\begin{thm}\label{thm_stone}
\cite{Stone77} Let $W$ be weight function over $\X \times \Y$. Suppose the following conditions hold for any distribution $\D$ over $\X \times \Y$.  Let $X$ be a random variable with distribution $\D_\X$, and $S = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \sim \D^n$. All expectations are taken over $X$ and $S$. 
\begin{enumerate}
	\item There is a constant $c$ such that, for every nonnegative measurable function $f$ satisfying $\mathbb{E} [f(X)] < \infty$, $$\mathbb{E} [\sum_1^n w_i^S(X)f(x_i)] \leq c \mathbb{E} [f(x)].$$
	\item For all $a > 0$, $$\lim_{n \to \infty} \mathbb{E}[\sum_1^n w_i^S(x)I_{||x_i - X|| > a}] = 0,$$ where $I_{||x_i - X|| > a}$ is an indicator variable. 
	\item $$\lim_{n \to \infty} \mathbb{E}[\max_{1 \leq i \leq n} w_i^S(X)] = 0.$$
\end{enumerate}
Then $W$ is consistent. 
\end{thm}
There are two main differences between Theorem \ref{thm_stone_cons} and Stone's theorem.
 \begin{enumerate}
 	\item Conditions 1. and 3. of Stone's theorem are no longer necessary. This is because $r$-separated distributions are well-separated and thus have simpler conditions for consistency. In fact, a slight modification of the arguments of~\cite{Stone77} shows that for $r$-separated distributions, condition 2. alone is sufficient for consistency.
 	\item Condition 2. is strengthened. Instead of requiring the weight of $x_i$'s outside of a given radius to go to $0$ for $X \sim \D$, we require the same to \textit{uniformly} hold over a ball centered at $X$. 
\end{enumerate}  
 


Theorem \ref{thm_stone_cons} provides a general condition that allows us to verify the $r$-consistency of non-parametric methods. We now show below that two common non-parametric algorithms -- $k_n$-nearest neighbors and kernel classifiers with rapidly decaying kernel functions -- satisfy the conditions of Theorem~\ref{thm_stone_cons}.
 
\begin{cor}\label{nn_sep_thm}
Let $\D$ be any $r$-separated distribution. Let $k_n$ be any sequence such that $\lim_{n \to \infty} \frac{k_n}{n} = 0$, and let $M$ be the $k_n$-nearest neighbors classifier on a sample $S \sim \D^n$. Then $M$ is \rcons\emph{ }with respect to $\D$. 
\end{cor}

\paragraph{\textbf{Remarks:}}
\begin{enumerate}
	\item Because the data distribution is $r$-separated, $k_n = 1$ will be $r$-consistent. Also observe that for $r$-separated distributions, $k_n = 1$ will converge towards the Bayes Optimal classifier.
	\item In general, $M$ converges towards the Bayes Optimal classifier provided that $k_n \to \infty$ in addition to $k_n /n \to 0$. This condition is not necessary for \rconsy -- because the distribution is $r$-separated. 
\end{enumerate}



We next show that kernel classifiers are also $r$-consistent on $r$-separated data distributions, provided the kernel function decreases rapidly enough. 

\begin{cor}\label{thm_kernel}
Let $W$ be a kernel classifier over $\X \times \Y$ constructed from $K$ and $h_n$. Suppose the following properties hold for $K$ and $h_n$.
\begin{enumerate}
	\item For any $c > 1$, $\lim_{x \to \infty} \frac{K(cx)}{K(x)} = 0.$
	\item $\lim_{n \to \infty} h_n = 0.$
\end{enumerate}
If $\D$ is an $r$-separated distribution over $\X \times \Y$, then $W$ is \rcons\emph{ }with respect to $\D$. 
\end{cor}

Observe that Condition 1. is satisfied for any $K(x)$ that decreases more rapidly than an inverse polynomial -- and is hence satisfied by most popular kernels like the Gaussian kernel. Is the condition on $K$ in Corollary~\ref{thm_kernel} necessary? The following example illustrates that a kernel classifier with any arbitrary $K$ is not necessarily $r$-consistent. This indicates that some sort of condition needs to be imposed on $K$ to ensure $r$-consistency; finding a tight necessary condition however is left for future work. 

 \paragraph{Example 3} Let $\X = [-1, 1]$ and let $\D$ be a distribution with $p_\D(-1, -1) = 0.1$ and $p_\D(1, 1) = 0.9$. Clearly, $\D$ is $0.3$-separated. Let $K(x) = e^{-\min(|x|, 0.2)^2}$. Let $h_n$ be any sequence with $\lim_{n \to \infty} h_n = 0$ and $\lim_{n \to \infty} nh_n = \infty$. Let $W$ be the weight classifier with input $S = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}$ such that $$w_i^S(x) = \frac{K(\frac{|x- x_i|}{h_n})}{\sum_{j=1}^n K(\frac{|x-x_j|}{h_n})}.$$ $W$ can be shown to satisfy all the conditions of Theorem \ref{thm_stone} (the proof is analogous to the case for a Gaussian Classifier), and is therefore consistent. However, $W$ does not learn a robust classifier on $\D$ for $r = 0.3$. 

Consider $x = -0.7$. For any $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \sim \D^n$, all $x_i$ will either be $-1$ or $1$. Therefore, since $K(|x - (-1)|) = K(|x - 1|)$, it follows that $w_i^S(x) = \frac{1}{n}$ for all $1 \leq i \leq n$. Since $x_i = 1$ with probability $0.9$, it follows that with high probability $x$ will be classified as $1$ which means that $f$, the output of $W$, is not robust at $x = -1$. Thus $f$ has astuteness at most $0.9$ which means that $W$ is \textit{not} \rcons\ for $r=0.3$. 

\section{General Distributions}\label{sec:general}


We next consider more general data distributions, where data from different classes may be close together in space, and may even overlap. Observe that unlike the $r$-separated case, here there may be no classifier with astuteness one. Thus, a natural question is: what does the optimally astute classifier look like, and how can we build non-parametric classifiers to this limit?

\subsection{The $r$-Optimal Classifier and Adversarial Pruning}

\cite{YRWC19} propose a large-sample limit -- called the $r$-optimal -- and show that it is analogous to the Bayes Optimal classifier for robustness. More specifically, given a data distribution $D$, to find the $r$-optimal classifier, we solve the following optimization problem.  

\begin{equation}\label{optim_prob}
\begin{split}
\max_{S_{+1}, S_{-1}} &\int_{x \in S_{+1}} p(y=+1|x)d\mu_{\D}(x) + \\
&\int_{x \in S_{-1}} p(y=-1|x)d\mu_{\D}(x) \\
&\text{ subject to } d(S_{+1}, S_{-1}) > 2r 
\end{split}
\end{equation}

Then, the $r$-optimal classifier is defined as follows. 

\begin{defn}
\cite{YRWC19} Fix $r, \D$. Let $S_{+1}^*$ and $S_{-1}^*$ be any optimizers of (\ref{optim_prob}). Then the $r$-optimal classifier, $\b_r$ is any classifier such that $\b_r(x) = j$ whenever $d(S_j^*, x) \leq r$. 
\end{defn}

\cite{YRWC19} show that the $r$-optimal classifier achieves the optimal astuteness -- out of all classifiers on the data distribution $\D$; hence, it is a robustness analogue to the Bayes Optimal Classifier. Therefore, for general distributions, the goal in robust classification is to find non-parametric algorithms that output classifiers that converge towards $\b_r$. 

To find robust classifiers, \cite{YRWC19} propose Adversarial Pruning -- a defense method that preprocesses the training data by making it better separated. More specifically, Adversarial Pruning takes as input a training dataset $S$ and a radius $r$, and finds the largest subset of the training set where differently labeled points are at least distance $2r$ apart. 


\begin{defn}
A set $S_r \subset \X \times \Y$ is said to be \textbf{$r$-separated} if for all $(x_1, y_1), (x_2, y_2) \in S_r$, if $y_1 \neq y_2$, then $d(x_1, x_2) > 2r$. To \textbf{adversarially prune} a set $S$ is to return its largest $r$-separated subset. We let $\ap(S, r)$ denote the result of adversarially pruning $S$.  
\end{defn}

Once an $r$-separated subset $S_r$ of the training set is found, a standard non-parametric method is trained on $S_r$.  While~\cite{YRWC19} show good empirical performance of such algorithms, no formal guarantees are provided. We next formally characterize when adversarial pruning followed by a non-parametric method results in a classifier that is provably $r$-consistent.

Specifically, we consider analyzing the general algorithm provided in Algorithm \ref{alg:gen}.

%\begin{algorithm}[tb]
%   \caption{\ga}
%   \label{alg:gen}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} $S \sim \D^n$, weight function $W$, 
%   robustness radius $r$
%   \STATE $S_r \leftarrow \ap(S, r)$
%   \STATE{\bfseries Output:} $W_{S_r}$
%\end{algorithmic}
%\end{algorithm}

\begin{algorithm}[H]
    \SetAlgoLined
    {\bfseries Input:} $S \sim \D^n$, weight function $W$, robustness radius $r$\;
    
    $S_r \leftarrow \ap(S, r)$\;
    
    {\bfseries Output:} $W_{S_r}$\;
    

\caption{\ga}\label{alg:gen}
\end{algorithm}

\subsection{Convergence Guarantees}

We begin with some notation. For any weight function $W$ and radius $r > 0$, we let $\ga(W,r)$ represent the weight function that outputs weights for $S \sim \D^n$ according to $\ga(S, W, r)$. In particular, this can be used to convert any weight function algorithm into a new weight function which takes robustness into account. A natural question is, for which weight functions $W$ is $\ga(W,r)$ \rcons? Our next theorem provides sufficient conditions for this.

\begin{thm}\label{thm_weight_general}
Let $W$ be a weight function over $\X \times \Y$, and let $\D$ be a distribution over $\X \times \Y$. Fix $r >0$. Let $S_r = \ap(S, r)$.  For convenience, relabel $x_i, y_i$ so that $S_r = \{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\}$. Suppose that for any $0 < a < b,$ 
\begin{equation*}\label{condition}
\lim_{n \to \infty} \mathbb{E}_{S \sim \D^n}\big [ \frac{1}{m} \sum_{i = 1}^m \sup_{x \in B(x_i, a)} \sum_{j = 1}^m w_j^{S_r}(x)I_{||x_j - x|| > b} \big] = 0. 
\end{equation*}
Then $\ga(W,r)$ is \rcons\emph{ }with respect to $\D$. 
\end{thm}

\paragraph{\textbf{Remark}:}
There are two important differences between the conditions in Theorem \ref{thm_weight_general} and Theorem~\ref{thm_stone_cons}.
\begin{enumerate}
	\item We replace $S$ with $S_r$.
	\item The expectation over $X \sim \D_\X$ is replaced with an average over $\{x_1, x_2, \dots, x_m\}$. The intuition here is that we are replacing $\D$ with a uniform distribution over $S_r$. While $\D$ may not be $r$-separated, the uniform distribution over $S_r$ is, and represents the region of points where our classifier is astute. 
\end{enumerate}

A natural question is what satisfies the conditions in Theorem~\ref{thm_weight_general}. We next show that $k_n$-nearest neighbors and kernel classifiers with rapidly decaying kernel functions continue to satisfy the conditions in Theorem \ref{thm_weight_general}; this means that these classifiers, when combined with Adversarial Pruning, will converge to $r$-optimal classifiers in the large sample limit.

\begin{cor}\label{thm_NN_gen}
Let $k_n$ be a sequence with $\lim_{n \to \infty} \frac{k_n}{n} = 0$, and let $M$ denote the $k_n$-nearest neighbor algorithm. Then for any $r > 0$, $\ga(M, r)$ is \rcons.
\end{cor}
\paragraph{\textbf{Remark}:} Corollary \ref{thm_NN_gen} gives a formal guarantee in the large sample limit for the modified nearest-neighbor algorithm proposed by \cite{YRWC19}.

\begin{cor}\label{thm_kern_gen}
Let $W$ be a kernel classifier over $\X \times \Y$ constructed from $K$ and $h_n$. Suppose the following properties hold for $K$ and $h_n$.
\begin{enumerate}
	\item For any $c > 1$, $\lim_{x \to \infty} \frac{K(cx)}{K(x)} = 0.$
	\item $\lim_{n \to \infty} h_n = 0.$
\end{enumerate}
Then for any $r > 0$, $\ga(W, r)$ is \rcons.
\end{cor}

Observe again that Condition 1. is satisfied by any $K$ that decreases more rapidly than an inverse polynomial kernel; it is thus satisfied by most popular kernels, such as the Gaussian kernel. 

\section{Validation}
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\subfloat[][Noiseless Histogram]{\includegraphics[width=.29\textwidth]{hist_noiseless_final}}\quad
%   \subfloat[][Noisy Histogram]{\includegraphics[width=.29\textwidth]{hist_noisy_final}} \quad
%   \subfloat[][Histogram trained on 500 samples]{\includegraphics[width=.29\textwidth]{visual500}}\\
%   \subfloat[][Noiseless 1-NN]{\includegraphics[width=.29\textwidth]{nn_noiseless_final}} \quad
%   \subfloat[][Noisy 1-NN]{\includegraphics[width=.29\textwidth]{nn_noisy_final}}\quad
%   \subfloat[][Histogram trained on 3000 samples]{\includegraphics[width=.29\textwidth]{visual3000}}
%\end{center}
%\caption{Empirical accuracy/astuteness of different classifiers as a function of training sample size. Accuracy is shown in green, astuteness in purple. Left : Noiseless Setting. Right: Noisy Setting. Top Row: Histogram Classifier, Bottom Row: 1-Nearest Neighbor}
%\label{fig:1}
%\vskip -0.2in
%\end{figure}

\begin{figure}
\begin{subfigure}{0.31\textwidth}
\includegraphics[width=\linewidth]{hist_noiseless_final}
%\caption{First subfigure} \label{fig:a}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.31\textwidth}
\includegraphics[width=\linewidth]{hist_noisy_final}
%\caption{Second subfigure} \label{fig:b}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.31\textwidth}
\includegraphics[width=\linewidth]{visual500}
%\caption{Fifth subfigure} \label{fig:e}
\end{subfigure}

\medskip
\begin{subfigure}{0.31\textwidth}
\includegraphics[width=\linewidth]{nn_noiseless_final}
%\caption{Third subfigure} \label{fig:c}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.31\textwidth}
\includegraphics[width=\linewidth]{nn_noisy_final}
%\caption{Fourth subfigure} \label{fig:d}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.31\textwidth}
 \includegraphics[width=\linewidth]{visual3000}
%\caption{Sixth subfigure} \label{fig:f}

\end{subfigure}

\caption{Empirical accuracy/astuteness of different classifiers as a function of training sample size. Accuracy is shown in green, astuteness in purple. Left : Noiseless Setting. Right: Noisy Setting. Top Row: Histogram Classifier, Bottom Row: 1-Nearest Neighbor} \label{fig:1}
\end{figure}

Our theoretical results are, by nature, large sample; we next validate how well they apply to the finite sample case by trying them out on a simple example. In particular, we ask the following question:

\begin{quote}
How does the robustness of non-parametric classifiers change with increasing sample size?
\end{quote}

This question is considered in the context of two simple non-parametric classifiers -- one nearest neighbor (which is guaranteed to be $r$-consistent) and histograms (which is not). To be able to measure performance with increasing data size, we look at a simple synthetic dataset -- the Half Moons. 

\subsection{Experimental Setup}

\paragraph{Classifiers and Dataset.} We consider two different classification algorithms -- one nearest neighbor (NN) and a Histogram Classifier (HC).  We use the Halfmoon dataset with two settings of the gaussian noise parameter $\sigma$, $\sigma = 0$ (Noiseless) and $\sigma =0.08$ (Noisy). For the Noiseless setting, observe that the data is already $0.1$-separated; for the Noisy setting, we use Adversarial Pruning (Algorithm~\ref{alg:gen}) with parameter $r = 0.1$ for both classification methods.

\paragraph{Performance Measure.} We evaluate robustness with respect to the $\ell_{\infty}$ metric, that is commonly used in the adversarial examples literature. Specifically, for each classifier, we calculate the {\em{empirical astuteness}}, which is the fraction of test examples on which it is astute.

Observe that computing the empirical astuteness of a classifier around an input $x$ amounts to finding the adversarial example that is {\em{closest to}} $x$ according to the $\ell_{\infty}$ norm. For the $1$-nearest neighbor, we do this using the optimal attack algorithm proposed by Yang et. al.~\cite{YRWC19}. For the histogram classifier, we use the optimal attack framework proposed by~\cite{YRWC19}, and show that the structure of the classifier can be exploited to solve the convex program efficiently. Details are in Appendix C.

We use an attack radius of $r = 0.1$ for the Noiseless setting, and $r = 0.09$ for the Noisy setting. For all classification algorithms, we plot the empirical astuteness as a function of the training set size. As a baseline, we also plot their standard accuracy on the test set. 

\subsection{Results}

The results are presented in Figure~\ref{fig:1}; the left two panels are for the Noiseless setting while the two center ones are for the Noisy setting.  

The results show that as predicted by our theory, for the Noiseless setting, the empirical astuteness of nearest neighbors converges to $1$ as the training set grows. For Histogram Classifiers, the astuteness converges to $0.5$ -- indicating that the classifier may grow less and less astute with higher sample size even for well-separated data. This is plausibly because the cell size induced by the histogram grows smaller with growing training data; thus, the classifier that outputs the default label $-1$ in empty cells is incorrect on adversarial examples that are close to a point with $+1$ label, but belongs to a different, empty cell. The rightmost panels in Figure~\ref{fig:1} provide a visual illustration of this process. 

For the Noisy setting, the empirical astuteness of adversarial pruning followed by nearest neighbors converges to $0.8$. For histograms with adversarial pruning, the astuteness converges to $0.7$, which is higher than the noiseless case but still clearly sub-optimal.

\subsection{Discussion}

Our results show that even though our theory is asymptotic, our predictions continue to be relevant in finite sample regimes. In particular, on well-separated data, nearest neighbors that we theoretically predict to be intrinsically robust is robust; histogram classifiers, which do not satisfy the conditions in Theorem~\ref{thm_stone_cons} are not. Our predictions continue to hold for data that is not well-separated. Nearest neighbors coupled with Adversarial
Pruning continues to be robust with growing sample size, while histograms continue to be non-robust. Thus our theory is confirmed by practice.

\section{Conclusion}

In conclusion, we rigorously analyze when non-parametric methods provide classifiers that are robust in the large sample limit. We provide a general condition that characterizes when non-parametric methods are robust on well-separated data, and show that Adversarial Pruning of~\cite{YRWC19} works on data that is not well-separated. 

Our results serve to provide a set of guidelines that can be used for designing non-parametric methods that are robust and accurate on well-separated data; additionally, we demonstrate that when data is not well-separated, preprocessing by adversarial pruning~\cite{YRWC19} does lead to optimally astute solutions in the large sample limit. 




%\input{chapters/chapter1/macros}
%\input{chapters/chapter1/introduction}
%\input{chapters/chapter1/related}
%\input{chapters/chapter1/definition}
%\input{chapters/chapter1/quantifying}
%\input{chapters/chapter1/visualizing}
%\input{chapters/chapter1/mitigation}