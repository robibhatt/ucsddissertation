\chapter*{Concluding Remarks}

This dissertation works towards addressing two specific issues in reliable machine learning, namely adversarial examples and data-copying. For adversarial-examples, we studied non-parametric classification along with linear classifiers and saw that in both cases, the robust setting leads to significant differences from the standard learning setting. For data-copying, we produced precise definition of what data-copying is along with an algorithm for detecting it. 

These two problems are two examples among many other issues (i.e. privacy, fairness) in reliable machine learning. This reflects that fact that word ``reliable" can many different things, some of which undoubtedly haven't been discovered. It is our belief that this necessitates careful conceptual work where the various sides of "reliable" and "trustworthy" are disentangled from each other into rigorous and measurable concepts, and we believe that our work provides progress towards doing this. 



%The above chapters provide a diverse set of examples of how privacy risks can be measured or mitigated in different scenarios. While highly different from each other, these examples all highlight the following three guiding principles for data privacy.
%
%\subsection*{No privacy definition is a `gold standard'}
%Differential privacy (DP) is often touted as the `gold standard' of data privacy. The cases studied in Chapters 3-5 challenge this by proposing entirely different privacy definitions for entirely different settings and risks. Chapter 3 proposes sentence privacy, which is DP-like but uses a different neighboring notion. Chapter 4, on the other hand, proposes a shuffling-based privacy definition that is almost orthogonal to DP. That is because the correlation adversary considered in that setting cannot be thwarted by DP alone. However, the broad population trends that we wish to learn are still accessible under our semi-random shuffling approach. Similarly, Chapter 5 analyzes the threat of correlation adversaries in the domain of location traces. Here, we see that a DP-based definition has to add \emph{more noise} in order to thwart these adversaries.
%
%Taken together, we see that sometimes DP definitions are effective, and other times they require one to choose between meaningful privacy and utility. If one `gold standard' definition were effective in all of these settings, we would not need to propose so many contrasting privacy definitions and methods. 
%
%\subsection*{No Free Lunch}
%The goal of data privacy is to allow the release of high-level information (\emph{e.g.} data distribution) while obscuring low-level information (\emph{e.g.} individuals' data features). It is natural to wonder whether it is possible to design a privacy definition under which we can release highly level information and defend against \emph{any} adversary. The answer is unequivocally, \emph{no}. This fact is known as the No-Free-Lunch theorem, made precise in \cite{Kifer}. The Theorem shows that releasing any information about a dataset that is useful to one person can be leveraged by an adversary to learn fine-grained information. The No-Free-Lunch theorem is instructive, because it shifts our attention from the question of whether we can provide air-tight privacy (impossible) to whether the adversaries our definition allows are \emph{realistic} in our setting. 
%
%The No-Free-Lunch principle is fundamental to the approaches of Chapters 4 and 5 in particular. Here, we propose novel privacy definitions that are adversary-focused. Note that in both of these papers, we consider limited classes of adversaries. As stated above, it is impossible to block the inferences of \emph{all adversaries} while still sharing useful information derived from the sensitive data. By practically evaluating what prior knowledge an adversary might have, like a correlation prior, we can formalize a privacy definition that gives strong guarantees in realistic settings. 
%
%\subsection*{Perfect is the enemy of good}
%Chapters 1 and 2 offer no formal privacy definition or provably private mechanism. Instead, they offer statistical tests to empirically evaluate a model's memorization of its training data, and thereby risk of exposing that data. In both chapters, we examine how model selection can effect the degree of memorization as detected by our tests. While our proposed test statistics do not confer any formal privacy guarantees, they guide practitioners towards models that memorize less. In many cases, our tests showed that it is possible to find models which have significantly less memorization at little to no cost in utility. 
%
%While formal privacy definitions are a valuable goal they make up only a small part of an ML practitioners privacy toolkit. To preserve privacy, we as researchers ought to put equal effort into methodical empirical privacy tests as we do formally private algorithms. These tests tend to be far more accessible to practitioners and allow them to significantly improve model privacy. Although empirical privacy tests are imperfect, the practical benefits to be gained by proposing them are undoubtedly a positive good. Do not let perfect privacy be the enemy of good privacy. 